# Frame Seek

This project explores a lightweight, on-device solution for intelligent video frame retrieval based on natural user queries. Given a video of arbitrary length, the system identifies and returns the most relevant frame that matches a user’s input — which can be a text description, an image of a person, or a combination of both.

## Architecture
We have used the following models, 
- **CLIP**: A fine-tuned version of OpenAI's CLIP has been used for video scene understanding.
- **FaceNet**: FaceNet is a face recognition model built on top of YOLOv8. We have used a fine-tuned FaceNet for face recognition.
- **MediaPipe**: MediaPipe is a face-detection service by Google and is used from its MLKit distribution.
- **ObjectBox**: A vector database, used to store vector embeddings generated by the individual model tokenizers.

## Run Locally
To run the project locally, 
- Clone the repository.
- Download the assets folder, containing the compressed model weights and tokenizer dictionaries, from [this link]([url](https://drive.google.com/drive/folders/1vu7BpwznG4XxKswEeXfTeXlIthmHJUv8?usp=sharing)).
- Place the assets folder in this path:
- Execute the project.

## Using the app
Follow the steps below to use this app. 
- Install and launch the app.
- Select a suitable video. Make sure the video has enough scenes and people to query.
- Press next and wait a few moments. Loading may take time the first time, since the frames are being processed.
- Once processed, the preview screen appears. Videos can be played on the app.
- Select an image or enter a text query, or both can be given in a single go.
- Results appear as timestamps below the query box.
- Press the result to seek the desired moment.

## Release
The latest APK package may be downloaded [from here]([url](https://drive.google.com/file/d/1eigecKbv7IhpLEv3LyLN_PwRWI5juOgu/view?usp=sharing)). 

